{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2091b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bceed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=250, help=\"number of epochs\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batch size\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimension of the latent space\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--ngf\", type=int, default=64, help=\"size of feature maps in generator\")\n",
    "parser.add_argument(\"--ndf\", type=int, default=64, help=\"size of feature maps in discriminator\")\n",
    "parser.add_argument(\"--multiplier\", type=float, default=0.6, help=\"weighting multiplier, which controls the relative contribution of generated data to the classifier training \")\n",
    "parser.add_argument(\"--threshold\", type=float, default=0.7, help=\"confidence threshold, which controls the quality of data to be used for classifier training\")\n",
    "parser.add_argument(\"--datasize\", type=float, default=0.1, help=\"atasize\")\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1daf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Classifier\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=opt.n_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.embDim = 128 * block.expansion\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(128 * block.expansion, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        emb = out.view(out.size(0), -1)\n",
    "        out = self.linear(emb)\n",
    "        return out\n",
    "    def get_embedding_dim(self):\n",
    "        return self.embDim\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a707e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditional GAN Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        \n",
    "        def block(in_feat, out_feat, kernel, stride, padding, bias=False):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, kernel, stride, padding, bias=False)]\n",
    "            layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.ReLU(True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, opt.ngf * 4, 4, 1, 0, bias=False),\n",
    "            *block(opt.ngf * 4, opt.ngf * 2, 4, 2, 1, bias=False),\n",
    "            *block(opt.ngf * 2, opt.ngf, 4, 2, 1, bias=False),\n",
    "            nn.ConvTranspose2d(opt.ngf,opt.channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        labels = self.label_emb(labels)\n",
    "        labels = torch.reshape(labels, (labels.size(0), labels.size(1), 1 , 1))\n",
    "        gen_input = torch.cat((labels, noise), 1)\n",
    "        img = self.model(gen_input)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01f3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditional GAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        self.linear_expand = nn.Linear(opt.n_classes, int(opt.img_size**2))\n",
    "    \n",
    "        def block(in_feat, out_feat, kernel, stride, padding, bias=False, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, kernel, stride, padding, bias=False)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.channels+1, opt.ndf, 4, 2, 1, bias=False, normalize=False),#(64,7,128,128)\n",
    "            *block(opt.ndf , opt.ndf * 2, 4, 2, 1, bias=False),\n",
    "            *block(opt.ndf * 2, opt.ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.Conv2d(opt.ndf* 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        labels = self.label_emb(labels)\n",
    "        labels = self.linear_expand(labels)\n",
    "        labels = torch.reshape(labels, (labels.size(0), 1, opt.img_size, opt.img_size))\n",
    "        d_in = torch.cat((img, labels), 1)\n",
    "        validity = self.model(d_in)\n",
    "        \n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380cd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "netC = ResNet18()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "netC.to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optC = torch.optim.Adam(netC.parameters(), lr=opt.lr, betas=(0.5, 0.999), weight_decay = 1e-3)\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78bf528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: datasets/SVHN\\train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(32),          # Resize to the same size\n",
    "        transforms.RandomCrop(32, padding=4),      # Crop to get square area\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),            \n",
    "        transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# regular data loaders\n",
    "batch_size = opt.batch_size\n",
    "trainset = datasets.SVHN(\"datasets/SVHN\", split='train', download = True, transform=transform)\n",
    "traindataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.SVHN(\"datasets/SVHN\", split='test', download = True, transform=transform_test)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "\n",
    "#create subsets\n",
    "dataSizeConstant = opt.datasize\n",
    "subTrainSet,_ = torch.utils.data.random_split(trainset, [int(dataSizeConstant*len(trainset)), len(trainset)-int(dataSizeConstant*len(trainset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b272dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(images, nclasses): \n",
    "    count = [0] * nclasses \n",
    "    for item in images: \n",
    "        count[item[1]] += 1 \n",
    "    weight_per_class = [0.] * nclasses \n",
    "    N = float(sum(count)) \n",
    "    for i in range(nclasses): \n",
    "        weight_per_class[i] = N/float(count[i]) \n",
    "    weight = [0] * len(images) \n",
    "    for idx, val in enumerate(images): \n",
    "        weight[idx] = weight_per_class[val[1]] \n",
    "    return weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de2f4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_sample_w(dataset, batch_size,weights):\n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    loader = DataLoader(dataset, shuffle=False, batch_size=batch_size,sampler=sampler, num_workers=8, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb543d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    netC.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    global gpred_labels, greal_labels\n",
    "    gpred_labels = torch.empty(0).to(device)\n",
    "    greal_labels = torch.empty(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = netC(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            gpred_labels = torch.cat((gpred_labels, torch.flatten(predicted)))\n",
    "            greal_labels = torch.cat((greal_labels, torch.flatten(labels)))\n",
    "    \n",
    "    print('Accuracy of the network on the test images: %f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ed1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for data in subTrainSet:\n",
    "        img, target = data\n",
    "        imgs.append((img, target))\n",
    "weights = make_weights_for_balanced_classes(imgs, 10)\n",
    "weights = torch.DoubleTensor(weights)\n",
    "\n",
    "subTrainLoader = gain_sample_w(subTrainSet, batch_size= batch_size, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6e97e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "C_losses = []\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(subTrainLoader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "        validlabel = Variable(FloatTensor(batch_size, ).fill_(1.0), requires_grad=False).to(device)\n",
    "        fakelabel = Variable(FloatTensor(batch_size, ).fill_(0.0), requires_grad=False).to(device)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "        labels = Variable(labels.type(LongTensor)).to(device)\n",
    "        \n",
    "        #---------------------------------\n",
    "        # (1) Update D network: \n",
    "        #---------------------------------\n",
    "        # Train with all-real data batch\n",
    "        discriminator.zero_grad()\n",
    "        # Forward pass real batch through D\n",
    "        output = discriminator(real_imgs, labels).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = adversarial_loss(output, validlabel)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "\n",
    "        # Train with all-fake data batch\n",
    "        # Generate batch of latent vectors and fake labels\n",
    "        z = torch.randn(batch_size, opt.latent_dim, 1, 1, device = device)\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "        # Generate fake image batch with G\n",
    "        fake =  generator(z, labels)\n",
    "        # Discriminate all fake batch with D\n",
    "        output = discriminator(fake.detach(), labels).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = adversarial_loss(output, fakelabel)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward(retain_graph = True)\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        #---------------------------------\n",
    "        # (2) Update G network: \n",
    "        #---------------------------------\n",
    "        generator.zero_grad()\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discriminator(fake, labels).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = adversarial_loss(output, validlabel)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        # Update G\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        #---------------------------------\n",
    "        #(3) Updata C network:\n",
    "        #---------------------------------\n",
    "        # train classifier on real data\n",
    "        \n",
    "        fake = fake.detach().clone()\n",
    "        predictions = netC(real_imgs)\n",
    "        realClassifierLoss = criterion(predictions, labels)\n",
    "        realClassifierLoss.backward(retain_graph = True)\n",
    "\n",
    "        optC.step()\n",
    "        optC.zero_grad()\n",
    "        \n",
    "        # train classifier on the synthesized data selected by the discriminator with a confidence being greater than or equal to β.\n",
    "        x = output.ge(opt.threshold)\n",
    "        Drealfake = fake[x]\n",
    "        Dreallabels = labels[x]\n",
    "        if Drealfake.shape[0]!=0:\n",
    "            \n",
    "            predictionsFake = netC(Drealfake)\n",
    "            fakeClassifierLoss = criterion(predictionsFake, Dreallabels)*opt.multiplier\n",
    "            fakeClassifierLoss.backward()\n",
    "\n",
    "            optC.step()\n",
    "            optC.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [C loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(subTrainLoader), errD.item(), errG.item(), realClassifierLoss.item()))\n",
    "            \n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        C_losses.append(realClassifierLoss.item())\n",
    "        \n",
    "        batches_done = epoch * len(subTrainLoader) + i\n",
    "        \n",
    "    validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "line1 =plt.plot(G_losses,'b-')\n",
    "line2 =plt.plot(D_losses,'r-')\n",
    "plt.legend(labels=['Generator_loss', ' Discriminator_loss'])\n",
    "plt.xlabel(\"iterations\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "line3 =plt.plot(C_losses,'g-')\n",
    "plt.title('Classifier loss')\n",
    "plt.legend(labels=['Classifier_loss'])\n",
    "plt.xlabel(\"iterations\")\n",
    "\n",
    "plt.savefig('Conditional_classifier_GAN.png',dpi=400,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094424b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(generator, f'generator_{opt.datasize}.pth') \n",
    "torch.save(discriminator, f'discriminator_{opt.datasize}.pth')\n",
    "torch.save(netC, f'netC_{opt.datasize}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "26c31387be611cb8781108affcc04e11d31a4e8edf26c9fc68831780550e6895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
